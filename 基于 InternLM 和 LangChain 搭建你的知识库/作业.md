# 配置环境
### 环境
最好选择A100(1/2)、激活环境、安装依赖。
### 模型下载
从本地`share`目录下拷贝`internlm-chat-7b`轻量级llm。
### LangChain环境
安装依赖、开源向量模型`Sentence Transformer`。

使用`huggingface`下载已训练好的`Sentence Transformer`模型参数。
### NLTK
`git clone`访问出现问题，手动下载、解压。

# 知识库搭建
### 数据收集
将选用以下开源仓库中所有的 markdown、txt 文件作为示例语料库：
```
git clone https://gitee.com/open-compass/opencompass.git
git clone https://gitee.com/InternLM/lmdeploy.git
git clone https://gitee.com/InternLM/xtuner.git
git clone https://gitee.com/InternLM/InternLM-XComposer.git
git clone https://gitee.com/InternLM/lagent.git
git clone https://gitee.com/InternLM/InternLM.git
```
### 加载数据
通过LangChain提供的FileLoader加载目标文件，解析出纯文本内容。

不同类型的目标文件对应着不同的FileLoader。
### 构建向量数据库
- 文本分块工具 `from langchain.text_splitter import RecursiveCharacterTextSplitter`
- 文本向量化 `from langchain.embeddings.huggingface import HuggingFaceEmbeddings`
- 向量数据库 `from langchain.vectorstores import Chroma`

# InternLM接入LangChain
- 基于本地部署的InternLM，继承LangChain的LLM类自定义一个子类（**※**）

# 构建检索问答链
- 通过 Chroma 以及上文定义的词向量模型来加载已构建的数据库。
- 实例化llm、prompt模板。
- 构建检索问答链。
```
from langchain.chains import RetrievalQA
qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectordb.as_retriever(),return_source_documents=True,chain_type_kwargs={"prompt":QA_CHAIN_PROMPT})
  ```
# 部署Web Demo
基于 Gradio 框架
![d8fd10c56bccfce51d13cfa0c45d409](https://github.com/GuoYiFantastic/InternLM_training_camp/assets/130634988/b73bd5ee-ee47-4d31-a701-313dc01868d3)

![be1d339a8e34ad5646b1fb700c66c15](https://github.com/GuoYiFantastic/InternLM_training_camp/assets/130634988/232de072-14d6-49e1-8138-294fa577a458)


